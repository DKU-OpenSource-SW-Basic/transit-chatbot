import torch
import torch.nn.functional as F
import re
from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification

# âœ… ì¡°ì‚¬ ì œê±° í•¨ìˆ˜
def remove_particle(word):
    particles = ['ì—ì„œ', 'ì—ê²Œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì—', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì™€', 'ê³¼', 'ë°–ì—', 'ë§Œ', 'ì¡°ì°¨', 'ê¹Œì§€', 'ë„', 'ì´ë‚˜', 'ë‚˜']
    for particle in particles:
        if word.endswith(particle):
            return word[:-len(particle)]
    return word

# âœ… ëª¨ë¸ ê²½ë¡œ
path = "finetuned_model"
slot_model_path = f"./{path}/slot"
intent_model_path = f"./{path}/intent"
tokenizer_path = f"./{path}/tokenizer"

# âœ… ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸
label_list = ['B-DIRECTION', 'B-LINE', 'B-ROUTE', 'B-STATION', 'B-TRANSPORT-BUS', 'B-TRANSPORT-SUBWAY', 'O']
intent_list = ['arrival_bus', 'arrival_subway', 'congestion', 'other']

# âœ… ì¥ì¹˜ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
slot_model = AutoModelForTokenClassification.from_pretrained(slot_model_path).to(device).eval()
intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_path).to(device).eval()

# âœ… ì˜ˆì¸¡ í•¨ìˆ˜
def predict(sentence, tokenizer, slot_model, intent_model, label_list, intent_list):
    words = sentence.strip().split()
    tokenized = tokenizer(
        words,
        is_split_into_words=True,
        return_tensors="pt",
        truncation=True,
        padding="max_length",
        max_length=128
    )

    for k in tokenized:
        tokenized[k] = tokenized[k].to(device)

    with torch.no_grad():
        # ì¸í…íŠ¸ ì˜ˆì¸¡
        intent_logits = intent_model(**tokenized).logits
        intent_probs = F.softmax(intent_logits, dim=1)[0]
        intent_pred_id = torch.argmax(intent_probs).item()
        intent_score = intent_probs[intent_pred_id].item()
        intent_label = intent_list[intent_pred_id]

        # í™•ì‹ ë„ ë‚®ìœ¼ë©´ ê¸°íƒ€ ì²˜ë¦¬
        if intent_score < 0.8:
            intent_label = "other"

        # ìŠ¬ë¡¯ íƒœê¹… ì˜ˆì¸¡
        slot_logits = slot_model(**tokenized).logits
        slot_probs = F.softmax(slot_logits, dim=2)[0]
        slot_preds = torch.argmax(slot_probs, dim=1).tolist()
        slot_scores = slot_probs[range(len(slot_preds)), slot_preds].tolist()

    input_ids = tokenized["input_ids"][0].cpu()
    word_ids = tokenized.word_ids(batch_index=0)

    print(f"\nğŸŸ¦ ë¬¸ì¥: {sentence}")
    print(f"ğŸ”¸ ì˜ˆì¸¡ ì¸í…íŠ¸: {intent_label}  (score: {intent_score:.4f})")

    # íƒœê¹… ê²°ê³¼ ì •ë¦¬
    extracted = {"B-STATION": [], "B-ROUTE": [], "B-LINE": []}
    word_to_tag = {}
    for idx, word_id in enumerate(word_ids):
        if word_id is None or input_ids[idx].item() in tokenizer.all_special_ids:
            continue
        if word_id not in word_to_tag:
            pred_id = slot_preds[idx]
            score = slot_scores[idx]
            word_to_tag[word_id] = (label_list[pred_id], score)

    #print("ë„ˆë¬´ ê¸¸ì–´ì ¸ì„œ ìŠ¬ë¡¯ ìƒëµ")
    #return

    print(f"ğŸ”¸ ìŠ¬ë¡¯ íƒœê¹…:")
    for i, word in enumerate(words):
        if i in word_to_tag:
            tag, score = word_to_tag[i]
            clean_word = remove_particle(word) if tag in extracted else word
            if tag in extracted:
                extracted[tag].append(clean_word)
            print(f"   {word:10} â†’ {tag:20} (score: {score:.4f})")
        else:
            print(f"   {word:10} â†’ [NO TAG]")
    
    if intent_label == "other":
        print("âš ï¸ ê¸°íƒ€ ì¸í…íŠ¸ë¡œ ë¶„ë¥˜ë˜ì–´ ì¶œë ¥ì„ ìƒëµí•©ë‹ˆë‹¤.\n")
        return

    # ğŸ¯ ì¸í…íŠ¸ì— ë”°ë¥¸ ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ¯ ì¸í…íŠ¸ë³„ í•„ìš”í•œ ì •ë³´:")
    missing = []

    if intent_label == "arrival_bus":
        station = ''.join(extracted["B-STATION"])
        route = ''.join(extracted["B-ROUTE"])
        print(f"  ğŸšŒ ì •ë¥˜ì¥(STATION): {station if station else 'ì—†ìŒ'}")
        print(f"  ğŸšŒ ë²„ìŠ¤ë²ˆí˜¸(ROUTE): {route if route else 'ì—†ìŒ'}")
        if not station:
            missing.append("ì •ë¥˜ì¥(STATION)")
        if not route:
            missing.append("ë²„ìŠ¤ë²ˆí˜¸(ROUTE)")

    elif intent_label in ["arrival_subway", "congestion"]:
        station = ''.join(extracted["B-STATION"])
        line = ''.join(extracted["B-LINE"])
        print(f"  ğŸš‡ ì§€í•˜ì² ì—­(STATION): {station if station else 'ì—†ìŒ'}")
        print(f"  ğŸš‡ ë…¸ì„ ëª…(LINE): {line if line else 'ì—†ìŒ'}")
        if not station:
            missing.append("ì§€í•˜ì² ì—­(STATION)")
        if not line:
            missing.append("ë…¸ì„ ëª…(LINE)")

    if missing:
        print(f"\nâš ï¸ í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {', '.join(missing)} ì´(ê°€) ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.\n")


# ì•„ë˜ ì½”ë“œë§Œ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•˜ë©´ ëœë‹¤.

# âœ… ì‚¬ìš©ì ì…ë ¥ ë°˜ë³µ
print("ğŸŸ¢ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”. 'exit', 'quit', 'q' ì…ë ¥ ì‹œ ì¢…ë£Œë©ë‹ˆë‹¤.\n")
while True:
    user_input = input("ğŸ’¬ ì…ë ¥ > ").strip()
    if user_input.lower() in ["exit", "quit", "q"]:
        print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    if user_input == "":
        continue
    predict(user_input, tokenizer, slot_model, intent_model, label_list, intent_list)